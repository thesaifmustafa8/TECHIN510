{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea1b194",
   "metadata": {},
   "source": [
    "# TECHIN 510\n",
    "## Milestone 2\n",
    "\n",
    "**Name**: Saif Mustafa\n",
    "\n",
    "**Email**: saifm@uw.edu\n",
    "\n",
    "**Student Number**: 1428039\n",
    "\n",
    "---\n",
    "\n",
    "For the second milestone, you will design and implement the interactive behaviors of your robot as specified below for the project you picked.\n",
    "\n",
    "Your robot's interactive behaviors can build on functionalities you have implemented in recent labs. For example, you can use the python server technique you developed in lab 5, the interactive robot face and speech recognition functionality you developed in Lab 6, and the sensor capturing functionalities you developed in Lab 7. \n",
    "\n",
    "The interaction should involve a dialog between the user and the robot, using a combination of input and output channels on the robot.\n",
    "\n",
    "**Channels for user input include:**\n",
    "- Speech\n",
    "- Touch-screen\n",
    "- Camera\n",
    "- Physical movement of device (motion/orientation sensing)\n",
    "\n",
    "**Channels for robot output include:**\n",
    "- Robot speech or sounds\n",
    "- Text or visualizations on screen\n",
    "- Gaze and facial expressions\n",
    "\n",
    "In addition, you will incorporate one of the visual recognition functionalities you developed in Milestone 1. You can do this by creating a python server using the code you create in Milestone 1 (similar to the server we created in Lab 5), developing visual processing functionality in JavaScript (similar to camera data processing in Lab 7) or using a web service that does visual processing (will be covered on Week 8). Preferably this functionality should be the same as one of the functionalities from Milestone 1, but you can also change your mind and pick a new functionality. \n",
    "\n",
    "To complete this milestone you will make a video that demonstrates your robot's capabilities. The video should include an end-to-end dialog between a person and the robot, while demonstrating the interactive behavior enabled by the visual processing capability (e.g. the robot pauses if the person disappears). Your video does not need to include narrative about the project, but should be edited to the essential parts the interaction (i.e., remove irrelevant parts of your recording). \n",
    "\n",
    "Complete this milestone by submitting a link to your video on Canvas by Dec 10, as well as any materials (code, data, physical prompts) that would allow us to recreate the demo shown in the video. Do not upload the video to Canvas; instead make sure it can be viewed on a browser through a link (Youtube, Vimeo, Dropbox, Google Drive, etcetera).\n",
    "\n",
    "---\n",
    "\n",
    "## Topic: Nike Exercise and Wellness Robot\n",
    "\n",
    "**Problem:** Many people who know the importance and potential benefits of exercising and meditation have a hard time motivating themselves to actually do them.\n",
    "\n",
    "**Proposed solution:** Social robots have been demonstrated to have the impact of a social accountability partner in committing to difficult behavioral changes. This project will explore this potential of social robots for exercise, yoga, and/or meditation motivation and guidance.\n",
    "\n",
    "**Prototype specifications:** The robot will have one user. The robot should interact with the user to introduce itself, meet its user, set user goals, and motivate the user to reach those goals. It should also guide the user through a sample exercise.\n",
    "\n",
    "**Image processing capabilities for this robot (Milestone 1):**\n",
    "- determine when a person is in front of the robot, \n",
    "- recognize whether the person is the owner of the robot, \n",
    "- determine the mood of the person, \n",
    "- determine when a person has completed an exercise, \n",
    "- determine when a functionality activation card (e.g. to start a specific exercise) is visually shown to the robot.\n",
    "\n",
    "**Interactive robot behaviors of this robot (Milestone 2):**\n",
    "\n",
    "Basic behaviors:\n",
    "- The robot walks the user through steps of an exercise routine, verbally explaining the different movements/actions. The robot moves onto the next movements/action upon verbal confirmation from the user, e.g. saying \"next move\".\n",
    "\n",
    "Behaviors that depend on image processing capabilities:\n",
    "- Robot automatically stops when the person is not detected\n",
    "- Robot acknowledges when the person has successfully performed the exercise and automatically moves on\n",
    "- Robot automatically moves to an exercise requested by the user through a card shown to the robot\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c63604",
   "metadata": {},
   "source": [
    "##  Installs / Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3666673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/cocoxu/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# installing opencv and mediapipe https://google.github.io/mediapipe/\n",
    "# !pip install mediapipe\n",
    "# !pip install opencv-python\n",
    "# !pip install tensorflow\n",
    "# !pip install deepface \n",
    "# !pip install tflearn\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import uuid\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# speech stuff\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import webbrowser\n",
    "import playsound\n",
    "from gtts import gTTS\n",
    "import random\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing model\n",
    "mp_pose = mp.solutions.pose # pose estimation model\n",
    "mp_hands = mp.solutions.hands # hand detection model (20 dots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6894f22",
   "metadata": {},
   "source": [
    "## Define training set + folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11945033",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['happy','sad','angry','surprised','neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877ee45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-3-bd93f35f0307>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset():\n",
    "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    def face_cropped(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        if faces is ():\n",
    "            return None\n",
    "        for (x,y,w,h) in faces:\n",
    "            cropped_face = img[y:y+h,x:x+w]\n",
    "        return cropped_face\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    \n",
    "    for emotion in emotions:\n",
    "        \n",
    "        img_id = 0\n",
    "        time.sleep(5)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if face_cropped(frame) is not None:\n",
    "                img_id +=1\n",
    "                face = cv2.resize(face_cropped(frame), (200,200))\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                file_name_path = \"data/\"+emotion+\".\"+str(img_id)+\".jpg\"\n",
    "                cv2.imwrite(file_name_path, face)\n",
    "                cv2.putText(face, str(img_id), (50,50), cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            1, (0,255,0), 2)\n",
    "                cv2.imshow(\"Cropped\", face)\n",
    "                if cv2.waitKey(1) == 13 or int(img_id)==250 :\n",
    "                    break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"All image data collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c899470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f204b94",
   "metadata": {},
   "source": [
    "## Label captured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f2cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_label(image_name):\n",
    "    emotion = image_name.split('.')[-3]\n",
    "    \n",
    "    #print(\"Emotion =\",emotion)\n",
    "    \n",
    "    if emotion == 'happy':\n",
    "        return np.array([1,0,0,0,0])\n",
    "    elif emotion == 'sad':\n",
    "        return np.array([0,1,0,0,0])\n",
    "    elif emotion == 'angry':\n",
    "        return np.array([0,0,1,0,0])\n",
    "    elif emotion == 'surprised':\n",
    "        return np.array([0,0,0,1,0])\n",
    "    else:\n",
    "        return np.array([0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea385148",
   "metadata": {},
   "source": [
    "## Create model data using captured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052da3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def my_data():\n",
    "    data = []\n",
    "    for img in tqdm(os.listdir(\"data\")):\n",
    "        path = os.path.join(\"data\",img)\n",
    "        img_data=cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_data=cv2.resize(img_data, (50,50))\n",
    "        data.append([np.array(img_data), my_label(img)])\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce00a955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:00<00:00, 1280.05it/s]\n"
     ]
    }
   ],
   "source": [
    "data = my_data() # remove DS.STORE file from github and rerun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07418169",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f6ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:875]\n",
    "test = data[875:1250]\n",
    "\n",
    "X_train, y_train, X_test, y_test = (np.array([i[0] for i in train]).reshape(-1,50,50,1), \n",
    "                                    [i[1] for i in train],\n",
    "                                    np.array([i[0] for i in test]).reshape(-1,50,50,1),\n",
    "                                    [i[1] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca5169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.34446\u001b[0m\u001b[0m | time: 4.668s\n",
      "| Adam | epoch: 020 | loss: 0.34446 - acc: 0.9497 -- iter: 832/875\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.31268\u001b[0m\u001b[0m | time: 5.971s\n",
      "| Adam | epoch: 020 | loss: 0.31268 - acc: 0.9547 | val_loss: 0.19034 - val_acc: 0.9440 -- iter: 875/875\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph() # deprecated function\n",
    "# https://stackoverflow.com/questions/40782271/attributeerror-module-tensorflow-has-no-attribute-reset-default-graph\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "convnet = input_data(shape=[50,50,1])\n",
    "convnet = conv_2d(convnet,32,5,activation='relu') # 32 filters with stride = 5\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,64,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,128,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,64,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,32,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 5, activation='softmax') # 5 emotions for output layer\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate = 0.001, loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_verbose=3)\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          n_epoch=20, \n",
    "          validation_set=(X_test, y_test),\n",
    "          show_metric=True,\n",
    "          run_id=\"emotion_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6e50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_drawing.draw_landmarks??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f173e4b",
   "metadata": {},
   "source": [
    "## Identifying joints\n",
    "\n",
    "Mediapipe all usable body landmarks.\n",
    "\n",
    "- 33 landmarks in total\n",
    "- index starting at 0\n",
    "- represent joints within the pose\n",
    "\n",
    "\n",
    "![Mediapipe body landmarks](https://google.github.io/mediapipe/images/mobile/pose_tracking_full_body_landmarks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4817cc1",
   "metadata": {},
   "source": [
    "## Angle calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247dc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get angle between any 3 given points\n",
    "def get_angle(p1, p2, p3):\n",
    "    \n",
    "    p1 = np.array(p1)\n",
    "    p2 = np.array(p2)\n",
    "    p3 = np.array(p3)\n",
    "    \n",
    "    radians = np.arctan2(p3[1]-p2[1], p3[0]-p2[0]) - np.arctan2(p1[1]-p2[1], p1[0]-p2[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fe9e7",
   "metadata": {},
   "source": [
    "## Identify left and right hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "727ec03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(index, hand, results):\n",
    "    output = None\n",
    "    for idx, classification in enumerate(results.multu_handedness):\n",
    "        if classification.classification[0].index == index:\n",
    "            \n",
    "            label = classification.classification[0].label\n",
    "            score = classification.classification[0].score\n",
    "            text = '{} {}'.format(label, rount(score, 2))\n",
    "            \n",
    "            coords = tuple(np.multiply(np.array((hand.landmark[mp_hands.HandLandmark.WRIST].x,\n",
    "                                                 hand.landmark[mp_hands.HandLandmark.WRIST].y)),\n",
    "                                       [1280,720]).astype(int))\n",
    "            \n",
    "            output = text, coords\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ce3c9",
   "metadata": {},
   "source": [
    "## Identify finger angles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea72d96",
   "metadata": {},
   "source": [
    "![Mediapipe hand landmarks](https://google.github.io/mediapipe/images/mobile/hand_landmarks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55c34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_list = [[8,6,5], # index\n",
    "              [12,10,9], # middle\n",
    "               [16,14,13],\n",
    "               [20,18,17],\n",
    "             [4,2,1]] # thumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93a4eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_finger_angles(image, results, joint_list):  \n",
    "    for hand in results.multi_hand_landmarks:\n",
    "        for joint in joint_list:\n",
    "            \n",
    "            a = np.array([hand.landmark[joint[0]].x, hand.landmark[joint[0]].y]) \n",
    "            b = np.array([hand.landmark[joint[1]].x, hand.landmark[joint[1]].y]) \n",
    "            c = np.array([hand.landmark[joint[2]].x, hand.landmark[joint[2]].y]) \n",
    "            \n",
    "            radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "            angle = np.abs(radians*100.0/np.pi)\n",
    "            \n",
    "#             if angle>180.0:\n",
    "#                 angle=360.0-angle\n",
    "                \n",
    "            cv2.putText(image, \n",
    "                       str(round(angle,2)), tuple(np.multiply(b, [1280,720]).astype(int)),\n",
    "                       cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,255,255),2,cv2.LINE_AA)\n",
    "            \n",
    "            #print(angle)\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29db7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finger_angle(image, results, joint): \n",
    "\n",
    "    for hand in results.multi_hand_landmarks:\n",
    "        a = np.array([hand.landmark[joint[0]].x, hand.landmark[joint[0]].y]) \n",
    "        b = np.array([hand.landmark[joint[1]].x, hand.landmark[joint[1]].y]) \n",
    "        c = np.array([hand.landmark[joint[2]].x, hand.landmark[joint[2]].y]) \n",
    "            \n",
    "        radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "        angle = np.abs(radians*100.0/np.pi)\n",
    "\n",
    "#             if angle>180.0:\n",
    "#                 angle=360.0-angle\n",
    "\n",
    "        cv2.putText(image, \n",
    "                   str(round(angle,2)), tuple(np.multiply(b, [1280,720]).astype(int)),\n",
    "                   cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "        #print(angle)\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f1f10",
   "metadata": {},
   "source": [
    "## Voice input and output methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f684f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio():\n",
    "    with sr.Microphone() as source:\n",
    "        audio = r.listen(source)\n",
    "        voice_data = ''\n",
    "        try:\n",
    "            voice_data = r.recognize_google(audio)\n",
    "            print(voice_data)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I didn't get that\")\n",
    "        except sr.RequestError:\n",
    "            print(\"Sorry, speech service is down\")\n",
    "        return voice_data\n",
    "            \n",
    "def alexis_speak(audio_string):\n",
    "    tts = gTTS(text=audio_string, lang='en')\n",
    "    r = random.randint(1,10000000)\n",
    "    audio_file = 'audio-' + str(r) + '.mp3'\n",
    "    tts.save(audio_file)\n",
    "    playsound.playsound(audio_file)\n",
    "    print(audio_string)\n",
    "    os.remove(audio_file)\n",
    "        \n",
    "def respond(voice_data):\n",
    "    if 'your name' in voice_data:\n",
    "        alexis_speak('My name is Nike')\n",
    "        \n",
    "    if 'calories' in voice_data:\n",
    "        alexis_speak('You have completed X reps, which means you have burned (X/10)*40 calories')\n",
    "        \n",
    "    if 'search' in voice_data:\n",
    "        url = 'https://assets.vogue.com/photos/5891e18123f9887c0e0e3f44/master/w_1600,c_limit/karlie-core-1.gif'\n",
    "        webbrowser.get().open(url)\n",
    "        #alexis_speak(\"Here's what I found\")\n",
    "    \n",
    "    voice_data=''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3e685",
   "metadata": {},
   "source": [
    "## Building a counter / tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c45b81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saif\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a1aa8d3cf167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Make pose detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# make hand detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mediapipe/python/solutions/pose.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mediapipe/python/solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    332\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;31m# output stream names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### https://docs.opencv.org/master/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Saif\")\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "    \n",
    "curl_count=0\n",
    "curl_flag_left=\"down\"\n",
    "curl_flag_right=\"False\"\n",
    "current_emotion = \"neutral\"\n",
    "x=0\n",
    "y=0\n",
    "h=0\n",
    "detected_emotion_index=0\n",
    "\n",
    "# voice stuff\n",
    "r = sr.Recognizer()\n",
    "voice_data = ''\n",
    "\n",
    "color_bg_ex1 = (255,255,255)\n",
    "color_bg_ex2 = (255,255,255)\n",
    "\n",
    "\n",
    "# alexis_speak(\"Today I'll be walking you through some exercises.\")\n",
    "# alexis_speak(\"To get started, please tell me what exercise you'd like\")\n",
    "\n",
    "# setting detection and tracking confidence    \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        #result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        faces = faceCascade.detectMultiScale(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),1.1,4)\n",
    "\n",
    "        for(x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0),2)\n",
    "            gray_img = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)\n",
    "            gray_img = cv2.resize(gray_img,(50,50),interpolation=cv2.INTER_AREA)\n",
    "            gray_img = gray_img.reshape(50,50,1) #  needs to be 50,50,1\n",
    "            # print(gray_img.shape) needs to be 50,50,1\n",
    "            detected_emotion_index = model.predict([gray_img])[0]\n",
    "            # print(np.argmax(emotion_detection))\n",
    "            # print(emotions[np.argmax(emotion_detection)])\n",
    "            x=x\n",
    "            y=y\n",
    "            h=h\n",
    "        \n",
    "        # if frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        \n",
    "        # Our operations on the frame come here\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make pose detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # make hand detection\n",
    "        results2 = hands.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract all joints\n",
    "        try:\n",
    "#             voice_data = record_audio()\n",
    "#             respond(voice_data)\n",
    "            \n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            if results2.multi_hand_landmarks:\n",
    "                #print(\"saw hands\")\n",
    "                for num, hand in enumerate(results2.multi_hand_landmarks):\n",
    "                    mp_drawing.draw_landmarks(image,\n",
    "                                              hand,\n",
    "                                              mp_hands.HAND_CONNECTIONS,\n",
    "                                              mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2), # dots\n",
    "                                              mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2))\n",
    "                    \n",
    "#                     if get_label(num, hand, results):\n",
    "#                         text, coord = get_label(num, hand, results)\n",
    "#                         cv2.putText(image, text, coord, cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2, cv2.LINE_AA)          \n",
    "                \n",
    "                #draw_finger_angles(image, results2, joint_list)\n",
    "            \n",
    "            if get_finger_angle(image, results2, joint_list[0]) >= 97 and get_finger_angle(image, results2, joint_list[0]) <= 102 and get_finger_angle(image, results2, joint_list[1]) < 97:\n",
    "                color_bg_ex1 = (69,255,213)\n",
    "                color_bg_ex2 = (255,255,255)\n",
    "                curl_count = 0\n",
    "            \n",
    "            if get_finger_angle(image, results2, joint_list[1]) >= 97 and get_finger_angle(image, results2, joint_list[1]) <= 102 and get_finger_angle(image, results2, joint_list[0]) >=97:\n",
    "                color_bg_ex2 = (69,255,213)\n",
    "                color_bg_ex1 = (255,255,255)\n",
    "                curl_count = 0\n",
    "            \n",
    "            # shoulder coordinates\n",
    "            shoulder_left = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            #print(\"Shoulder left =\",shoulder_left)\n",
    "            \n",
    "            shoulder_right = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            #print(\"Shoulder right =\",shoulder_right)\n",
    "            \n",
    "            # elbow coordinates\n",
    "            elbow_left = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            \n",
    "            elbow_right = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "\n",
    "            # wrist coordinates\n",
    "            wrist_left = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            wrist_right = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            \n",
    "            # calculate angle\n",
    "            # logic:\n",
    "            # - if angle is > 90, then it's a curl down\n",
    "            # - if angle < 90, the it's a curl up\n",
    "            # - some kind of counter that starts and countfs every 2 as 1 successful curl\n",
    "            # - visualize on screen\n",
    "            curl_angle_left = get_angle(shoulder_left, elbow_left, wrist_left)\n",
    "            #print(\"Curl angle left =\", curl_angle_left)\n",
    "            curl_angle_right = get_angle(shoulder_right, elbow_right, wrist_right)\n",
    "            #print(\"Curl angle right =\", curl_angle_right)\n",
    "            \n",
    "            # show angle at elbow\n",
    "            cv2.putText(image, \n",
    "                        str(round(curl_angle_right)), \n",
    "                        tuple(np.multiply(elbow_right, [1280,720]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(image, \n",
    "                        str(round(curl_angle_left)), \n",
    "                        tuple(np.multiply(elbow_left, [1280,720]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # print(\"Count = \",curl_count)\n",
    "            \n",
    "            if curl_angle_left > 160:\n",
    "                curl_flag_left = False # curl down\n",
    "            if curl_angle_left < 50 and not curl_flag_left:\n",
    "                curl_flag_left = True # curl up\n",
    "                curl_count+=1\n",
    "                #print(\"Count inside = \", curl_count)\n",
    "                \n",
    "            if curl_angle_right > 160:\n",
    "                curl_flag_right=False # curl down\n",
    "            if curl_angle_right < 50 and not curl_flag_right:\n",
    "                curl_flag_right=True # curl up\n",
    "                curl_count+=1\n",
    "                #print(\"Count inside = \", curl_count)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # display count\n",
    "        #cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "        #cv2.rectangle(image,(0,620),(1280,720),(0,0,0),-1)\n",
    "        \n",
    "        ## EXERCISE 1 TEXT\n",
    "        cv2.rectangle(image, (350,610), (560,710), color_bg_ex1, -1) # rgba(213,255,69,255)\n",
    "        cv2.putText(image, \"EXERCISE 1\", (360,635),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, .4, (120,116,124), 1, cv2.LINE_AA) # rgb(124,116,120)\n",
    "        cv2.putText(image, \"BICEP CURLS\", (360,675),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, .8, (0,0,0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        ## EXERCISE 2 TEXT   \n",
    "        cv2.rectangle(image, (740,610), (970,710), color_bg_ex2, -1) # rgba(213,255,69,255)\n",
    "        cv2.putText(image, \"EXERCISE 2\", (750,635),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, .4, (120,116,124), 1, cv2.LINE_AA) # rgb(124,116,120)\n",
    "        cv2.putText(image, \"PUSH UPS\", (750,675),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, .8, (0,0,0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        cv2.putText(image, \"NIKE FITNESS TRACKER\", (500,600),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(image, (600,610), (700,710), (69,255,213), -1) # rgba(213,255,69,255)\n",
    "        cv2.putText(image, \"REPS\", (635,635),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, .4, (120,116,124), 1, cv2.LINE_AA) # rgb(124,116,120)\n",
    "        cv2.putText(image, str(curl_count), (640,675),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,0), 2, cv2.LINE_AA) # rgb(124,116,120)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        completed = False\n",
    "        \n",
    "        if(curl_count==0):\n",
    "            cv2.putText(image, \n",
    "                    \"Let's see you do 30 rep curls!\",\n",
    "                    (570,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            if completed:\n",
    "                time.sleep(5)\n",
    "                completed=False\n",
    "        \n",
    "        if(curl_count>0 and curl_count<30):\n",
    "            cv2.putText(image, \n",
    "                    \"Nice form, keep going! \" + str(30-curl_count) + \" more reps to go\",\n",
    "                    (500,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "        if(curl_count>=30):\n",
    "            cv2.putText(image, \n",
    "                    \"Well done! Take a break!\",\n",
    "                    (570,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            curl_count=0\n",
    "            completed=True\n",
    "            color_bg_ex1 = (255,255,255)\n",
    "            color_bg_ex2 = (255,255,255)\n",
    "        \n",
    "        cv2.putText(image, \n",
    "                    \"Welcome Saif, M25\",\n",
    "                    (x,y),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(image, \n",
    "                    \"why u \" + emotions[np.argmax(detected_emotion_index)],\n",
    "                    (x+25,y+h+25),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        \n",
    "        # basic detection using the draw_landmarks utility\n",
    "        mp_drawing.draw_landmarks(image, # image\n",
    "                                  results.pose_landmarks, # coordinates\n",
    "                                  mp_pose.POSE_CONNECTIONS, # pose connections\n",
    "                                  mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2), # dots\n",
    "                                  mp_drawing.DrawingSpec(thickness=2, circle_radius=2)) # connections\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('NIKE WELLNESS DETECTOR', image)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c49b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e7fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
