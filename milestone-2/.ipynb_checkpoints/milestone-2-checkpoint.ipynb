{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea1b194",
   "metadata": {},
   "source": [
    "# TECHIN 510\n",
    "## Milestone 2\n",
    "\n",
    "**Name**: Saif Mustafa\n",
    "\n",
    "**Email**: saifm@uw.edu\n",
    "\n",
    "**Student Number**: 1428039\n",
    "\n",
    "---\n",
    "\n",
    "For the second milestone, you will design and implement the interactive behaviors of your robot as specified below for the project you picked.\n",
    "\n",
    "Your robot's interactive behaviors can build on functionalities you have implemented in recent labs. For example, you can use the python server technique you developed in lab 5, the interactive robot face and speech recognition functionality you developed in Lab 6, and the sensor capturing functionalities you developed in Lab 7. \n",
    "\n",
    "The interaction should involve a dialog between the user and the robot, using a combination of input and output channels on the robot.\n",
    "\n",
    "Channels for user input include:\n",
    "- Speech\n",
    "- Touch-screen\n",
    "- Camera\n",
    "- Physical movement of device (motion/orientation sensing)\n",
    "\n",
    "Channels for robot output include:\n",
    "- Robot speech or sounds\n",
    "- Text or visualizations on screen\n",
    "- Gaze and facial expressions\n",
    "\n",
    "In addition, you will incorporate one of the visual recognition functionalities you developed in Milestone 1. You can do this by creating a python server using the code you create in Milestone 1 (similar to the server we created in Lab 5), developing visual processing functionality in JavaScript (similar to camera data processing in Lab 7) or using a web service that does visual processing (will be covered on Week 8). Preferably this functionality should be the same as one of the functionalities from Milestone 1, but you can also change your mind and pick a new functionality. \n",
    "\n",
    "To complete this milestone you will make a video that demonstrates your robot's capabilities. The video should include an end-to-end dialog between a person and the robot, while demonstrating the interactive behavior enabled by the visual processing capability (e.g. the robot pauses if the person disappears). Your video does not need to include narrative about the project, but should be edited to the essential parts the interaction (i.e., remove irrelevant parts of your recording). \n",
    "\n",
    "Complete this milestone by submitting a link to your video on Canvas by Dec 10, as well as any materials (code, data, physical prompts) that would allow us to recreate the demo shown in the video. Do not upload the video to Canvas; instead make sure it can be viewed on a browser through a link (Youtube, Vimeo, Dropbox, Google Drive, etcetera).\n",
    "\n",
    "---\n",
    "\n",
    "## Topic: Nike Exercise and Wellness Robot\n",
    "\n",
    "**Problem:** Many people who know the importance and potential benefits of exercising and meditation have a hard time motivating themselves to actually do them.\n",
    "\n",
    "**Proposed solution:** Social robots have been demonstrated to have the impact of a social accountability partner in committing to difficult behavioral changes. This project will explore this potential of social robots for exercise, yoga, and/or meditation motivation and guidance.\n",
    "\n",
    "**Prototype specifications:** The robot will have one user. The robot should interact with the user to introduce itself, meet its user, set user goals, and motivate the user to reach those goals. It should also guide the user through a sample exercise.\n",
    "\n",
    "**Image processing capabilities for this robot (Milestone 1):**\n",
    "- determine when a person is in front of the robot, \n",
    "- recognize whether the person is the owner of the robot, \n",
    "- determine the mood of the person, \n",
    "- determine when a person has completed an exercise, \n",
    "- determine when a functionality activation card (e.g. to start a specific exercise) is visually shown to the robot.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c63604",
   "metadata": {},
   "source": [
    "##  Installs / Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3666673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing opencv and mediapipe https://google.github.io/mediapipe/\n",
    "# !pip install mediapipe\n",
    "# !pip install opencv-python\n",
    "# !pip install tensorflow\n",
    "# !pip install deepface \n",
    "# !pip install tflearn\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import uuid\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing model\n",
    "mp_pose = mp.solutions.pose # pose estimation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6894f22",
   "metadata": {},
   "source": [
    "## Define training set + folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11945033",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['happy','sad','angry','surprised','neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ee45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    def face_cropped(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        if faces is ():\n",
    "            return None\n",
    "        for (x,y,w,h) in faces:\n",
    "            cropped_face = img[y:y+h,x:x+w]\n",
    "        return cropped_face\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    \n",
    "    for emotion in emotions:\n",
    "        \n",
    "        img_id = 0\n",
    "        time.sleep(5)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if face_cropped(frame) is not None:\n",
    "                img_id +=1\n",
    "                face = cv2.resize(face_cropped(frame), (200,200))\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                file_name_path = \"data/\"+emotion+\".\"+str(img_id)+\".jpg\"\n",
    "                cv2.imwrite(file_name_path, face)\n",
    "                cv2.putText(face, str(img_id), (50,50), cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            1, (0,255,0), 2)\n",
    "                cv2.imshow(\"Cropped\", face)\n",
    "                if cv2.waitKey(1) == 13 or int(img_id)==250 :\n",
    "                    break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"All image data collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c899470",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f204b94",
   "metadata": {},
   "source": [
    "## Label captured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_label(image_name):\n",
    "    emotion = image_name.split('.')[-3]\n",
    "    \n",
    "    #print(\"Emotion =\",emotion)\n",
    "    \n",
    "    if emotion == 'happy':\n",
    "        return np.array([1,0,0,0,0])\n",
    "    elif emotion == 'sad':\n",
    "        return np.array([0,1,0,0,0])\n",
    "    elif emotion == 'angry':\n",
    "        return np.array([0,0,1,0,0])\n",
    "    elif emotion == 'surprised':\n",
    "        return np.array([0,0,0,1,0])\n",
    "    else:\n",
    "        return np.array([0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea385148",
   "metadata": {},
   "source": [
    "## Create model data using captured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052da3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def my_data():\n",
    "    data = []\n",
    "    for img in tqdm(os.listdir(\"data\")):\n",
    "        path = os.path.join(\"data\",img)\n",
    "        img_data=cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_data=cv2.resize(img_data, (50,50))\n",
    "        data.append([np.array(img_data), my_label(img)])\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_data() # remove DS.STORE file from github and rerun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07418169",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f6ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:875]\n",
    "test = data[875:1250]\n",
    "\n",
    "X_train, y_train, X_test, y_test = (np.array([i[0] for i in train]).reshape(-1,50,50,1), \n",
    "                                    [i[1] for i in train],\n",
    "                                    np.array([i[0] for i in test]).reshape(-1,50,50,1),\n",
    "                                    [i[1] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph() # deprecated function\n",
    "# https://stackoverflow.com/questions/40782271/attributeerror-module-tensorflow-has-no-attribute-reset-default-graph\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "convnet = input_data(shape=[50,50,1])\n",
    "convnet = conv_2d(convnet,32,5,activation='relu') # 32 filters with stride = 5\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,64,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,128,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,64,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,32,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 5, activation='softmax') # 5 emotions for output layer\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate = 0.001, loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_verbose=3)\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          n_epoch=20, \n",
    "          validation_set=(X_test, y_test),\n",
    "          show_metric=True,\n",
    "          run_id=\"emotion_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d228fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model Created:::::\n",
    "\n",
    "Training Step: 280  | total loss: 0.11289 | time: 4.963s\n",
    "| Adam | epoch: 020 | loss: 0.11289 - acc: 0.9788 | val_loss: 0.02673 - val_acc: 0.9947 -- iter: 875/875\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94164795",
   "metadata": {},
   "source": [
    "## Video Code + Basic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out my mac's webcam dimensions\n",
    "\n",
    "vcap = cv2.VideoCapture(0) # 0=camera\n",
    " \n",
    "if vcap.isOpened(): \n",
    "    # get vcap property \n",
    "    width  = vcap.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
    "    height = vcap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
    "\n",
    "    # it gives me 0.0 :/\n",
    "    fps = vcap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(\"width =\",width)\n",
    "    print(\"height =\",height)\n",
    "    \n",
    "    # 1280 x 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cba662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.opencv.org/master/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "    \n",
    "# setting detection and tracking confidence    \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # if frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        \n",
    "        # Our operations on the frame come here\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # basic detection using the draw_landmarks utility\n",
    "        mp_drawing.draw_landmarks(image, # image\n",
    "                                  results.pose_landmarks, # coordinates\n",
    "                                  mp_pose.POSE_CONNECTIONS, # pose connections\n",
    "                                  mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2), # dots\n",
    "                                  mp_drawing.DrawingSpec(thickness=2, circle_radius=2)) # connections\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('NIKE WELLNESS DETECTOR', image)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_drawing.draw_landmarks??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f173e4b",
   "metadata": {},
   "source": [
    "## Identifying joints\n",
    "\n",
    "Mediapipe all usable body landmarks.\n",
    "\n",
    "- 33 landmarks in total\n",
    "- index starting at 0\n",
    "- represent joints within the pose\n",
    "\n",
    "\n",
    "![Mediapipe body landmarks](https://google.github.io/mediapipe/images/mobile/pose_tracking_full_body_landmarks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8721b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings\n",
    "i = 0\n",
    "for dot in mp_pose.PoseLandmark:\n",
    "    print(i, \"=\", dot)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ae383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the specific coordinates to a given landmark:\n",
    "landmarks[mp_pose.PoseLandmark.NOSE.value] # nose\n",
    "# or\n",
    "landmarks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure curls -- need 11, 13, 15\n",
    "landmarks[11] # left shoulder\n",
    "landmarks[13] # left elbow\n",
    "landmarks[15] # left wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mouth example\n",
    "mouth = [landmarks[mp_pose.PoseLandmark.MOUTH_LEFT.value].x,\n",
    "         landmarks[mp_pose.PoseLandmark.MOUTH_RIGHT.value].y,\n",
    "         landmarks[mp_pose.PoseLandmark.MOUTH_RIGHT.value].z]\n",
    "\n",
    "mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle needed between shoulder, elbow, and wrist to determine curl\n",
    "# just doing x and y for now\n",
    "\n",
    "shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "            landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "\n",
    "elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "         landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "\n",
    "wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "         landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "\n",
    "shoulder, elbow, wrist\n",
    "\n",
    "get_angle(shoulder, elbow, wrist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4817cc1",
   "metadata": {},
   "source": [
    "## Angle calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247dc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get angle between any 3 given points\n",
    "def get_angle(p1, p2, p3):\n",
    "    \n",
    "    p1 = np.array(p1)\n",
    "    p2 = np.array(p2)\n",
    "    p3 = np.array(p3)\n",
    "    \n",
    "    radians = np.arctan2(p3[1]-p2[1], p3[0]-p2[0]) - np.arctan2(p1[1]-p2[1], p1[0]-p2[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3e685",
   "metadata": {},
   "source": [
    "## Building a counter / tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d357511",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### https://docs.opencv.org/master/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Saif\")\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "    \n",
    "curl_count=0\n",
    "curl_flag_left=\"down\"\n",
    "curl_flag_right=\"False\"\n",
    "current_emotion = \"neutral\"\n",
    "x=0\n",
    "y=0\n",
    "h=0\n",
    "detected_emotion_index=0\n",
    "\n",
    "# setting detection and tracking confidence    \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        #result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        faces = faceCascade.detectMultiScale(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),1.1,4)\n",
    "\n",
    "        for(x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0),2)\n",
    "            gray_img = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)\n",
    "            gray_img = cv2.resize(gray_img,(50,50),interpolation=cv2.INTER_AREA)\n",
    "            gray_img = gray_img.reshape(50,50,1) #  needs to be 50,50,1\n",
    "            # print(gray_img.shape) needs to be 50,50,1\n",
    "            detected_emotion_index = model.predict([gray_img])[0]\n",
    "            # print(np.argmax(emotion_detection))\n",
    "            # print(emotions[np.argmax(emotion_detection)])\n",
    "            x=x\n",
    "            y=y\n",
    "            h=h\n",
    "        \n",
    "        # if frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        \n",
    "        # Our operations on the frame come here\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make pose detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Make emotion detection\n",
    "#         gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         gray_img = cv2.resize(gray_img,(50,50),interpolation=cv2.INTER_AREA)\n",
    "#         gray_img = gray_img.reshape(50,50,1) #  needs to be 50,50,1\n",
    "#         # print(gray_img.shape) needs to be 50,50,1\n",
    "#         emotion_detection = model.predict([gray_img])[0]\n",
    "#         # print(np.argmax(emotion_detection))\n",
    "#         # print(emotions[np.argmax(emotion_detection)])\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract all joints\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # shoulder coordinates\n",
    "            shoulder_left = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            #print(\"Shoulder left =\",shoulder_left)\n",
    "            \n",
    "            shoulder_right = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            #print(\"Shoulder right =\",shoulder_right)\n",
    "            \n",
    "            # elbow coordinates\n",
    "            elbow_left = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            \n",
    "            elbow_right = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "\n",
    "            # wrist coordinates\n",
    "            wrist_left = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            wrist_right = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            \n",
    "            # calculate angle\n",
    "            # logic:\n",
    "            # - if angle is > 90, then it's a curl down\n",
    "            # - if angle < 90, the it's a curl up\n",
    "            # - some kind of counter that starts and countfs every 2 as 1 successful curl\n",
    "            # - visualize on screen\n",
    "            curl_angle_left = get_angle(shoulder_left, elbow_left, wrist_left)\n",
    "            #print(\"Curl angle left =\", curl_angle_left)\n",
    "            curl_angle_right = get_angle(shoulder_right, elbow_right, wrist_right)\n",
    "            #print(\"Curl angle right =\", curl_angle_right)\n",
    "            \n",
    "            # show angle at elbow\n",
    "            cv2.putText(image, \n",
    "                        str(round(curl_angle_right)), \n",
    "                        tuple(np.multiply(elbow_right, [1280,720]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(image, \n",
    "                        str(round(curl_angle_left)), \n",
    "                        tuple(np.multiply(elbow_left, [1280,720]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # print(\"Count = \",curl_count)\n",
    "            \n",
    "            if curl_angle_left > 160:\n",
    "                curl_flag_left = False # curl down\n",
    "            if curl_angle_left < 50 and not curl_flag_left:\n",
    "                curl_flag_left = True # curl up\n",
    "                curl_count+=1\n",
    "                #print(\"Count inside = \", curl_count)\n",
    "                \n",
    "            if curl_angle_right > 160:\n",
    "                curl_flag_right=False # curl down\n",
    "            if curl_angle_right < 50 and not curl_flag_right:\n",
    "                curl_flag_right=True # curl up\n",
    "                curl_count+=1\n",
    "                #print(\"Count inside = \", curl_count)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # display count\n",
    "        #cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "        #cv2.rectangle(image,(0,620),(1280,720),(0,0,0),-1)\n",
    "        cv2.putText(image, \"NIKE FITNESS TRACKER\", (500,600),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(image, (600,610), (700,710), (69,255,213), -1) # rgba(213,255,69,255)\n",
    "        cv2.putText(image, \"REPS\", (635,635),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, .4, (120,116,124), 1, cv2.LINE_AA) # rgb(124,116,120)\n",
    "        cv2.putText(image, str(curl_count), (640,675),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,0), 2, cv2.LINE_AA) # rgb(124,116,120)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        completed = False\n",
    "        \n",
    "        if(curl_count==0):\n",
    "            cv2.putText(image, \n",
    "                    \"Let's see you do 30 rep curls!\",\n",
    "                    (570,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            if completed:\n",
    "                time.sleep(5)\n",
    "                completed=False\n",
    "        \n",
    "        if(curl_count>0 and curl_count<30):\n",
    "            cv2.putText(image, \n",
    "                    \"Nice form, keep going! \" + str(30-curl_count) + \" more reps to go\",\n",
    "                    (500,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "        if(curl_count>=30):\n",
    "            cv2.putText(image, \n",
    "                    \"Well done! Take a break!\",\n",
    "                    (570,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            curl_count=0\n",
    "            completed=True\n",
    "        \n",
    "        cv2.putText(image, \n",
    "                    \"Welcome Saif, M25\",\n",
    "                    (x,y),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(image, \n",
    "                    \"why u \" + emotions[np.argmax(detected_emotion_index)],\n",
    "                    (x+25,y+h+25),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        \n",
    "        # basic detection using the draw_landmarks utility\n",
    "        mp_drawing.draw_landmarks(image, # image\n",
    "                                  results.pose_landmarks, # coordinates\n",
    "                                  mp_pose.POSE_CONNECTIONS, # pose connections\n",
    "                                  mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2), # dots\n",
    "                                  mp_drawing.DrawingSpec(thickness=2, circle_radius=2)) # connections\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('NIKE WELLNESS DETECTOR', image)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
