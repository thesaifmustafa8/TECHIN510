{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea1b194",
   "metadata": {},
   "source": [
    "# TECHIN 510\n",
    "## Milestone 1\n",
    "\n",
    "**Name**: Saif Mustafa\n",
    "\n",
    "**Email**: saifm@uw.edu\n",
    "\n",
    "**Student Number**: 1428039\n",
    "\n",
    "---\n",
    "\n",
    "As part of the first milestone for your projects, you will practice what you have learned so far by developing the visual recognition capabilities of the robot you choose to develop. As a first step read the project topics listed below and decide which topic you would like to work on. You will choose and implement two visual recognition functionalities from the specs listed in the project topic you choose.\n",
    "\n",
    "Your implementation will be in Python. You can use everything covered in class as well as any new functionality you discover yourself.\n",
    "\n",
    "You will complete this assignment by submitting a demo video and any materials (code, data, physical prompts) that would allow us to recreate your demo, on Canvas by Oct 29, 2021 (Friday). The video should clearly illustrate two functionalities. Make sure you demonstrate the functionality in varied scenarios (e.g., for face detection make sure you have at least two different people's faces in different poses relative to the camera). At this point, your video does not need to include narrative about the project. It can be as simple as a screenshot video showing the captured images from a camera with annotations that reflect the implemented functionalities (e.g. square around faces for face detection).\n",
    "\n",
    "---\n",
    "\n",
    "## Topic: Nike Exercise and Wellness Robot\n",
    "\n",
    "**Problem:** Many people who know the importance and potential benefits of exercising and meditation have a hard time motivating themselves to actually do them.\n",
    "\n",
    "**Proposed solution:** Social robots have been demonstrated to have the impact of a social accountability partner in committing to difficult behavioral changes. This project will explore this potential of social robots for exercise, yoga, and/or meditation motivation and guidance.\n",
    "\n",
    "**Prototype specifications:** The robot will have one user. The robot should interact with the user to introduce itself, meet its user, set user goals, and motivate the user to reach those goals. It should also guide the user through a sample exercise.\n",
    "\n",
    "**Image processing capabilities for this robot (Milestone 1):**\n",
    "- determine when a person is in front of the robot, \n",
    "- recognize whether the person is the owner of the robot, \n",
    "- determine the mood of the person, \n",
    "- determine when a person has completed an exercise, \n",
    "- determine when a functionality activation card (e.g. to start a specific exercise) is visually shown to the robot.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c63604",
   "metadata": {},
   "source": [
    "##  Installs / Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3666673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/cocoxu/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# installing opencv and mediapipe https://google.github.io/mediapipe/\n",
    "# !pip install mediapipe\n",
    "# !pip install opencv-python\n",
    "# !pip install tensorflow\n",
    "# !pip install deepface \n",
    "# !pip install tflearn\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import uuid\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing model\n",
    "mp_pose = mp.solutions.pose # pose estimation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6894f22",
   "metadata": {},
   "source": [
    "## Define training set + folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11945033",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['happy','sad','angry','surprised','neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877ee45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-4-bd93f35f0307>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset():\n",
    "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    def face_cropped(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        if faces is ():\n",
    "            return None\n",
    "        for (x,y,w,h) in faces:\n",
    "            cropped_face = img[y:y+h,x:x+w]\n",
    "        return cropped_face\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    \n",
    "    for emotion in emotions:\n",
    "        \n",
    "        img_id = 0\n",
    "        time.sleep(5)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if face_cropped(frame) is not None:\n",
    "                img_id +=1\n",
    "                face = cv2.resize(face_cropped(frame), (200,200))\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                file_name_path = \"data/\"+emotion+\".\"+str(img_id)+\".jpg\"\n",
    "                cv2.imwrite(file_name_path, face)\n",
    "                cv2.putText(face, str(img_id), (50,50), cv2.FONT_HERSHEY_DUPLEX, \n",
    "                            1, (0,255,0), 2)\n",
    "                cv2.imshow(\"Cropped\", face)\n",
    "                if cv2.waitKey(1) == 13 or int(img_id)==250 :\n",
    "                    break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"All image data collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c899470",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f204b94",
   "metadata": {},
   "source": [
    "## Label captured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f2cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_label(image_name):\n",
    "    emotion = image_name.split('.')[-3]\n",
    "    \n",
    "    #print(\"Emotion =\",emotion)\n",
    "    \n",
    "    if emotion == 'happy':\n",
    "        return np.array([1,0,0,0,0])\n",
    "    elif emotion == 'sad':\n",
    "        return np.array([0,1,0,0,0])\n",
    "    elif emotion == 'angry':\n",
    "        return np.array([0,0,1,0,0])\n",
    "    elif emotion == 'surprised':\n",
    "        return np.array([0,0,0,1,0])\n",
    "    else:\n",
    "        return np.array([0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea385148",
   "metadata": {},
   "source": [
    "## Create model data using captured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052da3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def my_data():\n",
    "    data = []\n",
    "    for img in tqdm(os.listdir(\"data\")):\n",
    "        path = os.path.join(\"data\",img)\n",
    "        img_data=cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_data=cv2.resize(img_data, (50,50))\n",
    "        data.append([np.array(img_data), my_label(img)])\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce00a955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:01<00:00, 786.23it/s]\n"
     ]
    }
   ],
   "source": [
    "data = my_data() # remove DS.STORE file from github and rerun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07418169",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f6ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:875]\n",
    "test = data[875:1250]\n",
    "\n",
    "X_train, y_train, X_test, y_test = (np.array([i[0] for i in train]).reshape(-1,50,50,1), \n",
    "                                    [i[1] for i in train],\n",
    "                                    np.array([i[0] for i in test]).reshape(-1,50,50,1),\n",
    "                                    [i[1] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca5169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.33422\u001b[0m\u001b[0m | time: 3.882s\n",
      "| Adam | epoch: 020 | loss: 0.33422 - acc: 0.9549 -- iter: 832/875\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.30310\u001b[0m\u001b[0m | time: 5.182s\n",
      "| Adam | epoch: 020 | loss: 0.30310 - acc: 0.9594 | val_loss: 0.06271 - val_acc: 0.9787 -- iter: 875/875\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph() # deprecated function\n",
    "# https://stackoverflow.com/questions/40782271/attributeerror-module-tensorflow-has-no-attribute-reset-default-graph\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "convnet = input_data(shape=[50,50,1])\n",
    "convnet = conv_2d(convnet,32,5,activation='relu') # 32 filters with stride = 5\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,64,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,128,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,64,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "convnet = conv_2d(convnet,32,5,activation='relu')\n",
    "convnet = max_pool_2d(convnet,5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 5, activation='softmax') # 5 emotions for output layer\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate = 0.001, loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_verbose=3)\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          n_epoch=20, \n",
    "          validation_set=(X_test, y_test),\n",
    "          show_metric=True,\n",
    "          run_id=\"emotion_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d228fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model Created:::::\n",
    "\n",
    "Training Step: 280  | total loss: 0.11289 | time: 4.963s\n",
    "| Adam | epoch: 020 | loss: 0.11289 - acc: 0.9788 | val_loss: 0.02673 - val_acc: 0.9947 -- iter: 875/875\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94164795",
   "metadata": {},
   "source": [
    "## Video Code + Basic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out my mac's webcam dimensions\n",
    "\n",
    "vcap = cv2.VideoCapture(0) # 0=camera\n",
    " \n",
    "if vcap.isOpened(): \n",
    "    # get vcap property \n",
    "    width  = vcap.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
    "    height = vcap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
    "\n",
    "    # it gives me 0.0 :/\n",
    "    fps = vcap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(\"width =\",width)\n",
    "    print(\"height =\",height)\n",
    "    \n",
    "    # 1280 x 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cba662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.opencv.org/master/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "    \n",
    "# setting detection and tracking confidence    \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # if frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        \n",
    "        # Our operations on the frame come here\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # basic detection using the draw_landmarks utility\n",
    "        mp_drawing.draw_landmarks(image, # image\n",
    "                                  results.pose_landmarks, # coordinates\n",
    "                                  mp_pose.POSE_CONNECTIONS, # pose connections\n",
    "                                  mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2), # dots\n",
    "                                  mp_drawing.DrawingSpec(thickness=2, circle_radius=2)) # connections\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('NIKE WELLNESS DETECTOR', image)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_drawing.draw_landmarks??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f173e4b",
   "metadata": {},
   "source": [
    "## Identifying joints\n",
    "\n",
    "Mediapipe all usable body landmarks.\n",
    "\n",
    "- 33 landmarks in total\n",
    "- index starting at 0\n",
    "- represent joints within the pose\n",
    "\n",
    "\n",
    "![Mediapipe body landmarks](https://google.github.io/mediapipe/images/mobile/pose_tracking_full_body_landmarks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8721b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 = PoseLandmark.NOSE\n",
      "1 = PoseLandmark.LEFT_EYE_INNER\n",
      "2 = PoseLandmark.LEFT_EYE\n",
      "3 = PoseLandmark.LEFT_EYE_OUTER\n",
      "4 = PoseLandmark.RIGHT_EYE_INNER\n",
      "5 = PoseLandmark.RIGHT_EYE\n",
      "6 = PoseLandmark.RIGHT_EYE_OUTER\n",
      "7 = PoseLandmark.LEFT_EAR\n",
      "8 = PoseLandmark.RIGHT_EAR\n",
      "9 = PoseLandmark.MOUTH_LEFT\n",
      "10 = PoseLandmark.MOUTH_RIGHT\n",
      "11 = PoseLandmark.LEFT_SHOULDER\n",
      "12 = PoseLandmark.RIGHT_SHOULDER\n",
      "13 = PoseLandmark.LEFT_ELBOW\n",
      "14 = PoseLandmark.RIGHT_ELBOW\n",
      "15 = PoseLandmark.LEFT_WRIST\n",
      "16 = PoseLandmark.RIGHT_WRIST\n",
      "17 = PoseLandmark.LEFT_PINKY\n",
      "18 = PoseLandmark.RIGHT_PINKY\n",
      "19 = PoseLandmark.LEFT_INDEX\n",
      "20 = PoseLandmark.RIGHT_INDEX\n",
      "21 = PoseLandmark.LEFT_THUMB\n",
      "22 = PoseLandmark.RIGHT_THUMB\n",
      "23 = PoseLandmark.LEFT_HIP\n",
      "24 = PoseLandmark.RIGHT_HIP\n",
      "25 = PoseLandmark.LEFT_KNEE\n",
      "26 = PoseLandmark.RIGHT_KNEE\n",
      "27 = PoseLandmark.LEFT_ANKLE\n",
      "28 = PoseLandmark.RIGHT_ANKLE\n",
      "29 = PoseLandmark.LEFT_HEEL\n",
      "30 = PoseLandmark.RIGHT_HEEL\n",
      "31 = PoseLandmark.LEFT_FOOT_INDEX\n",
      "32 = PoseLandmark.RIGHT_FOOT_INDEX\n"
     ]
    }
   ],
   "source": [
    "# Mappings\n",
    "i = 0\n",
    "for dot in mp_pose.PoseLandmark:\n",
    "    print(i, \"=\", dot)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ae383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the specific coordinates to a given landmark:\n",
    "landmarks[mp_pose.PoseLandmark.NOSE.value] # nose\n",
    "# or\n",
    "landmarks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure curls -- need 11, 13, 15\n",
    "landmarks[11] # left shoulder\n",
    "landmarks[13] # left elbow\n",
    "landmarks[15] # left wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mouth example\n",
    "mouth = [landmarks[mp_pose.PoseLandmark.MOUTH_LEFT.value].x,\n",
    "         landmarks[mp_pose.PoseLandmark.MOUTH_RIGHT.value].y,\n",
    "         landmarks[mp_pose.PoseLandmark.MOUTH_RIGHT.value].z]\n",
    "\n",
    "mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle needed between shoulder, elbow, and wrist to determine curl\n",
    "# just doing x and y for now\n",
    "\n",
    "shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "            landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "\n",
    "elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "         landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "\n",
    "wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "         landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "\n",
    "shoulder, elbow, wrist\n",
    "\n",
    "get_angle(shoulder, elbow, wrist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4817cc1",
   "metadata": {},
   "source": [
    "## Angle calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "247dc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get angle between any 3 given points\n",
    "def get_angle(p1, p2, p3):\n",
    "    \n",
    "    p1 = np.array(p1)\n",
    "    p2 = np.array(p2)\n",
    "    p3 = np.array(p3)\n",
    "    \n",
    "    radians = np.arctan2(p3[1]-p2[1], p3[0]-p2[0]) - np.arctan2(p1[1]-p2[1], p1[0]-p2[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3e685",
   "metadata": {},
   "source": [
    "## Building a counter / tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b8202c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saif\n"
     ]
    }
   ],
   "source": [
    "##### https://docs.opencv.org/master/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\"Saif\")\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "    \n",
    "curl_count=0\n",
    "curl_flag_left=\"down\"\n",
    "curl_flag_right=\"False\"\n",
    "current_emotion = \"neutral\"\n",
    "    \n",
    "# setting detection and tracking confidence    \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        #result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        faces = faceCascade.detectMultiScale(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),1.1,4)\n",
    "\n",
    "        for(x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0),2)\n",
    "            gray_img = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)\n",
    "            gray_img = cv2.resize(gray_img,(50,50),interpolation=cv2.INTER_AREA)\n",
    "            gray_img = gray_img.reshape(50,50,1) #  needs to be 50,50,1\n",
    "            # print(gray_img.shape) needs to be 50,50,1\n",
    "            emotion_detection = model.predict([gray_img])[0]\n",
    "            # print(np.argmax(emotion_detection))\n",
    "            # print(emotions[np.argmax(emotion_detection)])\n",
    "        \n",
    "        # if frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        \n",
    "        # Our operations on the frame come here\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Make pose detection\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Make emotion detection\n",
    "#         gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         gray_img = cv2.resize(gray_img,(50,50),interpolation=cv2.INTER_AREA)\n",
    "#         gray_img = gray_img.reshape(50,50,1) #  needs to be 50,50,1\n",
    "#         # print(gray_img.shape) needs to be 50,50,1\n",
    "#         emotion_detection = model.predict([gray_img])[0]\n",
    "#         # print(np.argmax(emotion_detection))\n",
    "#         # print(emotions[np.argmax(emotion_detection)])\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract all joints\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # shoulder coordinates\n",
    "            shoulder_left = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            #print(\"Shoulder left =\",shoulder_left)\n",
    "            \n",
    "            shoulder_right = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            #print(\"Shoulder right =\",shoulder_right)\n",
    "            \n",
    "            # elbow coordinates\n",
    "            elbow_left = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            \n",
    "            elbow_right = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "\n",
    "            # wrist coordinates\n",
    "            wrist_left = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            \n",
    "            wrist_right = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            \n",
    "            # calculate angle\n",
    "            # logic:\n",
    "            # - if angle is > 90, then it's a curl down\n",
    "            # - if angle < 90, the it's a curl up\n",
    "            # - some kind of counter that starts and countfs every 2 as 1 successful curl\n",
    "            # - visualize on screen\n",
    "            curl_angle_left = get_angle(shoulder_left, elbow_left, wrist_left)\n",
    "            #print(\"Curl angle left =\", curl_angle_left)\n",
    "            curl_angle_right = get_angle(shoulder_right, elbow_right, wrist_right)\n",
    "            #print(\"Curl angle right =\", curl_angle_right)\n",
    "            \n",
    "            # show angle at elbow\n",
    "            cv2.putText(image, \n",
    "                        str(round(curl_angle_right)), \n",
    "                        tuple(np.multiply(elbow_right, [1280,720]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(image, \n",
    "                        str(round(curl_angle_left)), \n",
    "                        tuple(np.multiply(elbow_left, [1280,720]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # print(\"Count = \",curl_count)\n",
    "            \n",
    "            if curl_angle_left > 160:\n",
    "                curl_flag_left = False # curl down\n",
    "            if curl_angle_left < 50 and not curl_flag_left:\n",
    "                curl_flag_left = True # curl up\n",
    "                curl_count+=1\n",
    "                #print(\"Count inside = \", curl_count)\n",
    "                \n",
    "            if curl_angle_right > 160:\n",
    "                curl_flag_right=False # curl down\n",
    "            if curl_angle_right < 50 and not curl_flag_right:\n",
    "                curl_flag_right=True # curl up\n",
    "                curl_count+=1\n",
    "                #print(\"Count inside = \", curl_count)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # display count\n",
    "        #cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "        #cv2.rectangle(image,(0,620),(1280,720),(0,0,0),-1)\n",
    "        cv2.putText(image, \"NIKE FITNESS TRACKER\", (500,600),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(image, (600,610), (700,710), (69,255,213), -1) # rgba(213,255,69,255)\n",
    "        cv2.putText(image, \"REPS\", (635,635),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, .4, (120,116,124), 1, cv2.LINE_AA) # rgb(124,116,120)\n",
    "        cv2.putText(image, str(curl_count), (640,675),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,0), 2, cv2.LINE_AA) # rgb(124,116,120)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        completed = False\n",
    "        \n",
    "        if(curl_count==0):\n",
    "            cv2.putText(image, \n",
    "                    \"Let's see you do 30 rep curls!\",\n",
    "                    (570,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            if completed:\n",
    "                time.sleep(5)\n",
    "                completed=False\n",
    "        \n",
    "        if(curl_count>0 and curl_count<30):\n",
    "            cv2.putText(image, \n",
    "                    \"Nice form, keep going! \" + str(30-curl_count) + \" more reps to go\",\n",
    "                    (500,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "        if(curl_count>=30):\n",
    "            cv2.putText(image, \n",
    "                    \"Well done! Take a break!\",\n",
    "                    (570,30),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            curl_count=0\n",
    "            completed=True\n",
    "        \n",
    "        cv2.putText(image, \n",
    "                    \"Welcome Saif, M25\",\n",
    "                    (x,y),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.75, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.putText(image, \n",
    "                    \"why u \" + emotions[np.argmax(emotion_detection)],\n",
    "                    (x+25,y+h+25),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        \n",
    "        # basic detection using the draw_landmarks utility\n",
    "        mp_drawing.draw_landmarks(image, # image\n",
    "                                  results.pose_landmarks, # coordinates\n",
    "                                  mp_pose.POSE_CONNECTIONS, # pose connections\n",
    "                                  mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2), # dots\n",
    "                                  mp_drawing.DrawingSpec(thickness=2, circle_radius=2)) # connections\n",
    "        \n",
    "                \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('NIKE WELLNESS DETECTOR', image)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
