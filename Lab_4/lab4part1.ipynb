{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NseBwf7G1t9P"
      },
      "source": [
        "# LAB 4: Image Classification\n",
        "\n",
        "## Step 1: Create project/folder and download data\n",
        "\n",
        "Download [lab-4.zip](https://drive.google.com/file/d/1vjt-_R1YnpIkxoytykU_ZZ5CLNP7fVgi/view?usp=sharing) and unzip it. Place the `train/` and `test/` folders into `lab-4/` folder you create on your Google drive. These folders include images of five 'symbol' cards as seen from a small robot's camera. Also copy `lab4.ipynb` into the `lab-4/`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zis5CUsHvYOv"
      },
      "source": [
        "**Name:** Saif Mustafa\n",
        "\n",
        "**Student Number:** 1428093"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqgTPZYE3AgX"
      },
      "source": [
        "## Step 2: Implement feature extraction\n",
        "\n",
        "Below is a skeleton code for an image classification class called ImageClassifier as well as code for creating, training, and testing a classifier with the provided data sets. The three functions you will need to implement are indicated with comments in the code.\n",
        "\n",
        "The first one of these is `extract_image_features` which should return a Numpy array that contains the features that represent the image. Before extracting any features, you should apply Gaussian blurring to your images to get rid of random sensor noice that is common in many lower cost cameras. For this, look into the filters module of scikit-image. Then explore at least two different types of features provided in the feature module of scikit-image. Inspect the size of the feature vectors generated by different methods and what the features look like for different images from the training or testing set. \n",
        "\n",
        "You will not yet get a good sense of how well each feature performs in allowing the classifiers to discriminate between different images. Hence, keep your code for extracting different features around until you explore classification performance in the next step. You can do that by duplicating the function with different names for different features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S99lms1i1BPE",
        "outputId": "dc648816-6806-4d5a-8a9a-448e11028e9a"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "from sklearn import svm, metrics\n",
        "from skimage import io, feature, filters, exposure, color, exposure\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class FeatureExtractor:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.classifier = None\n",
        "        self.folder = '/content/drive/MyDrive/Colab Notebooks/lab-4/lab4-data/'\n",
        "\n",
        "    def imread_convert(self, f):\n",
        "        return io.imread(f).astype(np.uint8)\n",
        "\n",
        "    def save_classifier(self):\n",
        "        joblib.dump(self.classifier, self.folder + 'classifier.joblib')\n",
        "\n",
        "    def load_data_from_folder(self, dir):\n",
        "        # read all images into an image collection\n",
        "        ic = io.ImageCollection(self.folder + dir + '*.bmp',\n",
        "                                load_func=self.imread_convert)\n",
        "\n",
        "        # create one large array of image data\n",
        "        data = io.concatenate_images(ic)\n",
        "        \n",
        "        # extract labels from image names\n",
        "        labels = np.array(ic.files)\n",
        "        for i, f in enumerate(labels):\n",
        "            m = re.search('_', f)\n",
        "            labels[i] = (f[len(dir):m.start()]).split('/')[-1]\n",
        "        \n",
        "        return(data,labels)\n",
        "    \n",
        "    def extract_image_features(self, data, feature):\n",
        "        ########################################################################\n",
        "        ############################ YOUR CODE HERE ############################\n",
        "        ########################################################################\n",
        "\n",
        "        vectors = [] # will store 1D vectorized images\n",
        "\n",
        "        for img in data:\n",
        "          img = filters.gaussian(img, multichannel=False) # apply gaussian\n",
        "          gray_img = color.rgb2gray(img) # apply grayscale\n",
        "          vectors.append(feature(gray_img).flatten()) # flatten -> makes it a 1D array\n",
        "\n",
        "        return vectors\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPG2wS285ytL"
      },
      "source": [
        "Here is the code for creating an instance of the class, loading the data, and extracting the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQumlfXtt5hr",
        "outputId": "6e6a2a49-e16b-466d-be41-db871f677589"
      },
      "source": [
        "img_clf = FeatureExtractor()\n",
        "\n",
        "# load images\n",
        "print('Loading training set...')\n",
        "(train_raw, train_labels) = img_clf.load_data_from_folder('train/')\n",
        "print('Loading testing set...')\n",
        "(test_raw, test_labels) = img_clf.load_data_from_folder('test/')\n",
        "print()\n",
        "\n",
        "# convert images into features, example using hog features\n",
        "print('Extracting HOG features...')\n",
        "train_data_hog = img_clf.extract_image_features(train_raw, feature.hog)\n",
        "test_data_hog = img_clf.extract_image_features(test_raw, feature.hog)\n",
        "\n",
        "# repeat with at least one other feature type...\n",
        "print('Extracting Canny features...')\n",
        "train_data_canny = img_clf.extract_image_features(train_raw, feature.canny)\n",
        "test_data_canny = img_clf.extract_image_features(test_raw, feature.canny)\n",
        "\n",
        "# repeat with at least one other feature type...\n",
        "print('Extracting Peak Local Max features...')\n",
        "train_data_plm = img_clf.extract_image_features(train_raw, feature.peak_local_max)\n",
        "test_data_plm = img_clf.extract_image_features(test_raw, feature.peak_local_max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training set...\n",
            "Loading testing set...\n",
            "\n",
            "Extracting HOG features...\n",
            "Extracting Canny features...\n",
            "Extracting Peak Local Max features...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHW9Rr1O5peP",
        "outputId": "0c3e7e45-0625-41bc-f15c-407effa54439"
      },
      "source": [
        "# Inspecting the features.\n",
        "print('HOG Feature Vector')\n",
        "print(train_data_hog[0])\n",
        "print(f'Vector Size: {len(train_data_hog[0])}') # Vector Size: 86184\n",
        "print() \n",
        "\n",
        "print('##########################################################################')\n",
        "print()\n",
        "\n",
        "# repeat with at least one other feature type...\n",
        "\n",
        "# Inspecting the features.\n",
        "print('Canny Feature Vector')\n",
        "print(train_data_canny[0])\n",
        "print(f'Vector Size: {len(train_data_canny[0])}') # Vector Size: 720\n",
        "print()\n",
        "\n",
        "print('##########################################################################')\n",
        "print()\n",
        "\n",
        "# Inspecting the features.\n",
        "print('Peak Local Max Vector')\n",
        "print(train_data_plm[0])\n",
        "print(f'Vector Size: {len(train_data_plm[0])}')\n",
        "print()\n",
        "\n",
        "print('##########################################################################')\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOG Feature Vector\n",
            "[0.04775442 0.03536229 0.03683505 ... 0.01790003 0.04655878 0.04953976]\n",
            "Vector Size: 86184\n",
            "\n",
            "##########################################################################\n",
            "\n",
            "Canny Feature Vector\n",
            "[False False False ... False False False]\n",
            "Vector Size: 76800\n",
            "\n",
            "##########################################################################\n",
            "\n",
            "Peak Local Max Vector\n",
            "[237  35 236 ...  78   1  77]\n",
            "Vector Size: 2866\n",
            "\n",
            "##########################################################################\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co_et4A-58X5"
      },
      "source": [
        "## Step 3: Implement classifier training and testing\n",
        "Next, implement the `train_classifier` and `predict_labels` functions for at least two different types of classifiers. After this, the second cell below shoul then produce performance results of the classifier.\n",
        "\n",
        "Explore the performance of at least **two feature types** and at least **two classifiers** (i.e. at least four different combinations) in terms of classification **performance** on the test set. Update the code to display the **F1 score** on the test set for the **four different combinations** with informative prompts. Then the code should display the **detailed performance** (confusion matrix, accuracy, F1 score) that is already displayed only for the best performing combination of features and classifier. Also make sure the best performing classifier is saved onto your drive for the next step of the lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHzU_eUg5K7V"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn.model_selection import cross_val_score \n",
        "\n",
        "# Continue implementation of the ImageClassifier class in this cell\n",
        "class ImageClassifier(FeatureExtractor):\n",
        "\n",
        "    def train_classifier(self, train_data, train_labels, classifier=KNeighborsClassifier):\n",
        "        \n",
        "        self.classifier = classifier()\n",
        "        self.classifier.fit(train_data, train_labels)\n",
        "\n",
        "    def predict_labels(self, data):\n",
        "        \n",
        "        predicted_labels = self.classifier.predict(data)\n",
        "        \n",
        "        return predicted_labels       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIYNOKWtX2-H",
        "outputId": "a946da26-8974-4f97-97d2-a8c38fa3789a"
      },
      "source": [
        "print(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['drone' 'drone' 'drone' 'drone' 'drone' 'hands' 'hands' 'hands' 'hands'\n",
            " 'hands' 'inspection' 'inspection' 'inspection' 'inspection' 'inspection'\n",
            " 'none' 'none' 'none' 'none' 'none' 'order' 'order' 'order' 'order'\n",
            " 'order' 'place' 'place' 'place' 'place' 'place' 'plane' 'plane' 'plane'\n",
            " 'plane' 'plane' 'truck' 'truck' 'truck' 'truck' 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzu432Q86AMR"
      },
      "source": [
        "Re-run initialization and feature extraction, then train/test the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44IFo86c6J-G",
        "outputId": "d0e61be4-b580-492e-f094-2670df8d4995"
      },
      "source": [
        "from skimage.feature import hog, canny, peak_local_max\n",
        "# https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
        "\n",
        "img_clf = ImageClassifier()\n",
        "\n",
        "\n",
        "# create list of list containing features you want to run through along with any\n",
        "# associated metadata\n",
        "\n",
        "# list of features to test, peak_local_max giving too many errors\n",
        "features = [hog,canny]\n",
        "\n",
        "# list of classifiers\n",
        "classifiers = [KNeighborsClassifier, MLPClassifier]\n",
        "\n",
        "# Variable to hold the best F1 score\n",
        "best_f1 = 0\n",
        "\n",
        "# Loop through the different combinantions of features and classifiers\n",
        "for f in features:\n",
        "\n",
        "  # train and test data before entering classifier loop\n",
        "  train_data = img_clf.extract_image_features(train_raw, f)\n",
        "  test_data = img_clf.extract_image_features(test_raw, f)\n",
        "\n",
        "  for c in classifiers:  \n",
        "\n",
        "    # Train model\n",
        "    train = img_clf.train_classifier(train_data, train_labels, c)\n",
        "\n",
        "    # Test model\n",
        "    predicted_labels = img_clf.predict_labels(test_data)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    confusion_matrix = skmetrics.confusion_matrix(test_labels, predicted_labels)\n",
        "    print(\"Classifer:\", c)\n",
        "    print(\"Feature: \", f)\n",
        "    print(\"Confusion Matrix:\") \n",
        "    print(confusion_matrix)\n",
        "\n",
        "    # Print test accuracy\n",
        "    print(\"Accuracy:\", skmetrics.accuracy_score(test_labels, predicted_labels))\n",
        "\n",
        "    # Print test F1 score\n",
        "    print(\"F1 Score:\", skmetrics.f1_score(test_labels, predicted_labels, average='weighted'))\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"-----\")\n",
        "    print(\"\")\n",
        "    \n",
        "    # Check if last model is better than the current best one and save it\n",
        "    if(skmetrics.f1_score(test_labels, predicted_labels, average='weighted') > best_f1):\n",
        "      best_f1 = skmetrics.f1_score(test_labels, predicted_labels, average='weighted')\n",
        "      img_clf.save_classifier()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifer: <class 'sklearn.neighbors._classification.KNeighborsClassifier'>\n",
            "Feature:  <function hog at 0x7f1ed3ab09e0>\n",
            "Confusion Matrix:\n",
            "[[4 0 0 0 0 0 0 1]\n",
            " [0 4 1 0 0 0 0 0]\n",
            " [0 0 5 0 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 1 0 0 2 2 0 0]\n",
            " [0 0 0 0 0 2 3 0]\n",
            " [0 0 0 1 0 0 4 0]\n",
            " [1 0 0 0 0 1 0 3]]\n",
            "Accuracy: 0.725\n",
            "F1 Score: 0.7153679653679654\n",
            "\n",
            "-----\n",
            "\n",
            "Classifer: <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
            "Feature:  <function hog at 0x7f1ed3ab09e0>\n",
            "Confusion Matrix:\n",
            "[[1 0 0 0 4 0 0 0]\n",
            " [0 5 0 0 0 0 0 0]\n",
            " [0 1 4 0 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 0 5 0 0 0]\n",
            " [0 0 0 0 0 5 0 0]\n",
            " [0 0 0 1 0 0 4 0]\n",
            " [0 0 0 0 1 1 0 3]]\n",
            "Accuracy: 0.8\n",
            "F1 Score: 0.781881313131313\n",
            "\n",
            "-----\n",
            "\n",
            "Classifer: <class 'sklearn.neighbors._classification.KNeighborsClassifier'>\n",
            "Feature:  <function canny at 0x7f1ed8c33f80>\n",
            "Confusion Matrix:\n",
            "[[0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [0 0 0 5 0 0 0 0]]\n",
            "Accuracy: 0.125\n",
            "F1 Score: 0.02777777777777778\n",
            "\n",
            "-----\n",
            "\n",
            "Classifer: <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
            "Feature:  <function canny at 0x7f1ed8c33f80>\n",
            "Confusion Matrix:\n",
            "[[4 0 0 0 0 0 1 0]\n",
            " [0 1 3 1 0 0 0 0]\n",
            " [0 0 4 0 0 0 1 0]\n",
            " [0 0 0 5 0 0 0 0]\n",
            " [2 2 0 0 0 0 1 0]\n",
            " [0 2 2 1 0 0 0 0]\n",
            " [2 0 0 1 0 0 2 0]\n",
            " [3 0 0 1 0 0 1 0]]\n",
            "Accuracy: 0.4\n",
            "F1 Score: 0.29366883116883125\n",
            "\n",
            "-----\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzct0QRVrcnk"
      },
      "source": [
        "print(\"Train data: {}\".format(train_raw.shape))\n",
        "print(\"Test data: {}\".format(test_raw.shape))\n",
        "print(\"Train labels: {}\".format(train_labels.shape))\n",
        "print(\"Test labels: {}\".format(test_labels.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxpmL5GSZL3p"
      },
      "source": [
        "# TODO: Save the best model using the `save_classifier` method of the `FeatureExtractor` class.\n",
        "# done in previous loop, ignore:\n",
        "# img_clf.save_classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF_Qu0I9O0zl"
      },
      "source": [
        "## Step 4: Transfer classifier to your camera\n",
        "\n",
        "Next you will apply your trained classifier directly onto images captured by your webcam. Since this is our last lab using Python we would like to give you the opportunity to install Python and Jupyter notebooks on your machine, and do this part of the lab as a conventional Python script. However, if you would rather not do that at the moment, you still have the option of doing this part with Colab Notebooks.\n",
        "\n",
        "* **Option 1:** If you would like to do this part locally, first follow the Python Installation guidelines. Then download the skeleton camera capture script `camera.py` and test it out. Then update this script as described below and submit this script. Make sure you note in lab4.ipynb that you chose this option.\n",
        "* **Option 2:** If you would like to do this part in Colab Notebooks, add your code below. Check out [`camera.ipynb`](https://colab.research.google.com/drive/1IfHqK83dDVyxsQzQwsnxc4CUrRUoL9y8) for sample code for capturing camera images in Colab notebooks.\n",
        "\n",
        "Your code should first load the classifier you saved in Step 3 with the following line (already implemented in the sample code):\n",
        "\n",
        "`classifier = joblib.load('classifier.joblib')`\n",
        "\n",
        "The script should then go into a loop where it (1) captures a new image from the camera, (2) processes the image to make it grayscale and filter the noise, (3) extracts the features like you did in Step 2 using the right set of features for your trained classifier, (4) detects whether the image contains one of the seven images using the trained classifier, and (5) displays the detected class name on the image in every iteration. You can use the images printed on paper provided by the TAs to test your script with your camera."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "JxAIUMPmPRY-",
        "outputId": "0ea355c4-e7f2-4d11-c2f0-7b9c8fb001ec"
      },
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from IPython.display import clear_output\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io as io2\n",
        "from PIL import Image\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from skimage.feature import hog\n",
        "\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "############################\n",
        "####### CAMERA CODE ########\n",
        "############################\n",
        "\n",
        "VIDEO_HTML = \"\"\"\n",
        "<video autoplay\n",
        " width=%d height=%d style='cursor: pointer;'></video>\n",
        "<script>\n",
        "\n",
        "var video = document.querySelector('video')\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({ video: true })\n",
        "  .then(stream=> video.srcObject = stream)\n",
        "\n",
        "function getFrame() {\n",
        "    var canvas = document.createElement('canvas')\n",
        "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
        "    canvas.width = w\n",
        "    canvas.height = h\n",
        "    canvas.getContext('2d')\n",
        "          .drawImage(video, 0, 0, w, h)\n",
        "    return canvas.toDataURL('image/jpeg', 0.8)\n",
        "}\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Make sure the pictures taken by your camera match the size of the \n",
        "# training set – 320px x 240px\n",
        "def start_camera(filename='photo.jpg', quality=0.8, size=(320,240)):\n",
        "  display(HTML(VIDEO_HTML % (size[0],size[1])))\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8, size=(320,240)):\n",
        "  data = eval_js('getFrame()')\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  f = io2.BytesIO(binary)\n",
        "  return np.asarray(Image.open(f))\n",
        "\n",
        "############################\n",
        "############################\n",
        "############################\n",
        "\n",
        "class ImageClassifer_webcam(ImageClassifier):\n",
        "  \n",
        "  def load_classifier(self):\n",
        "    print('Loading classifier...')\n",
        "    self.classifier = joblib.load(self.folder + 'classifier.joblib')\n",
        "\n",
        "# Create a new instance of the classifier\n",
        "img_clf = ImageClassifer_webcam()\n",
        "\n",
        "# Load the previously saved model\n",
        "img_clf.load_classifier()\n",
        "\n",
        "print(\"Classifier =\", img_clf.classifier)\n",
        "\n",
        "# Start camera and wait for it to \"warm up\"\n",
        "start_camera()\n",
        "time.sleep(3)\n",
        "\n",
        "while(True):\n",
        "  img = take_photo() # click\n",
        "\n",
        "  ##############################################################################\n",
        "  ############################ YOUR CODE HERE ##################################\n",
        "  ##############################################################################\n",
        "\n",
        "  # Convert to grayscale\n",
        "  g_img = color.rgb2gray(img)\n",
        "\n",
        "  # Extract features from the image just caputred\n",
        "  extracted = hog(g_img)\n",
        "  \n",
        "  # Show frame and prediction\n",
        "  pred = img_clf.predict_labels(extracted.reshape(1,-1))\n",
        "  print(\"Prediction:\",pred)\n",
        "\n",
        "  time.sleep(1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading classifier...\n",
            "Classifier = MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
            "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
            "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
            "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "              warm_start=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<video autoplay\n",
              " width=320 height=240 style='cursor: pointer;'></video>\n",
              "<script>\n",
              "\n",
              "var video = document.querySelector('video')\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({ video: true })\n",
              "  .then(stream=> video.srcObject = stream)\n",
              "\n",
              "function getFrame() {\n",
              "    var canvas = document.createElement('canvas')\n",
              "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
              "    canvas.width = w\n",
              "    canvas.height = h\n",
              "    canvas.getContext('2d')\n",
              "          .drawImage(video, 0, 0, w, h)\n",
              "    return canvas.toDataURL('image/jpeg', 0.8)\n",
              "}\n",
              "\n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: ['truck']\n",
            "Prediction: ['order']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-98c301308d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hRob3q_Qedo"
      },
      "source": [
        "## (Optional) Improve classifier\n",
        "\n",
        "You will notice that your classifier is prone to errors when tested with your camera. Part of the reason is that the images were collected from a different camera. You can try improving the performance of your camera image classifier by re-training your classifier with images collected from your camera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxJ9SZubQjKl"
      },
      "source": [
        "## Step 5: Submit your code on Canvas\n",
        "\n",
        "Complete this lab by submitting a link to your updated Colab Notebook (and if you chose Option 1 in Step 4, by uploading your updated `camera.py`) on Canvas, by Oct 22 Tuesday, 11:59pm. We will test your code by running it and inspecting the classification results to make sure: \n",
        "* A comparison of  at least four combinations of feature types and classifiers were made\n",
        "* A reasonable classification performance was achieved with the best combination (higher than random chance).\n",
        "\n",
        "We will test your camera image classification code by running it and showing it the seven different printed images to check that more than half of the images can be recognized correctly in some configuration relative to the camera.\n",
        "\n",
        "See Canvas for a grading rubric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyEj0yzbfiI5"
      },
      "source": [
        "# Scratchpad (Please ignore)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm43Xd9x1sQt"
      },
      "source": [
        "print(img.reshape(1,-1).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "ODlLAdM3UOT4",
        "outputId": "912c47d7-7303-43bc-a92a-26df272c1589"
      },
      "source": [
        "img = '/content/drive/MyDrive/Colab Notebooks/lab-4/lab4-data/train/drone_10T105759665802.bmp'\n",
        "img = plt.imread(img)\n",
        "g_img = color.rgb2gray(img)\n",
        "\n",
        "  # Extract features from the image just caputred\n",
        "extracted = feature.hog(g_img)\n",
        "gaussian = filters.gaussian(extracted).flatten()\n",
        "\n",
        "# Show frame and prediction\n",
        "pred = img_clf.classifier.predict(gaussian.reshape(1,-1))\n",
        "print(\"Prediction:\",pred)\n",
        "plt.imshow(img, cmap ='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8b0b1a39eb17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Show frame and prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaussian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    969\u001b[0m         \"\"\"\n\u001b[1;32m    970\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    683\u001b[0m                                          layer_units[i + 1])))\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 104\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 76800 is different from 86184)"
          ]
        }
      ]
    }
  ]
}